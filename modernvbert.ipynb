{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5abccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_emb: 768 first3: [-0.004450473003089428, 0.04333425685763359, -0.03766343742609024]\n",
      "image_emb: 768 first3: [0.006753838155418634, -0.0058644916862249374, -0.046822741627693176]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from colpali_engine.models import BiModernVBert, BiModernVBertProcessor\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "model_id = \"ModernVBERT/modernvbert-embed\"\n",
    "\n",
    "processor = BiModernVBertProcessor.from_pretrained(model_id)\n",
    "model = BiModernVBert.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "image = Image.open(hf_hub_download(\"HuggingFaceTB/SmolVLM\", \"example_images/rococo.jpg\", repo_type=\"space\"))\n",
    "text = \"This is a text\"\n",
    "\n",
    "# Prepare inputs\n",
    "text_inputs = processor.process_texts([text]).to(get_device())\n",
    "image_inputs = processor.process_images([image]).to(get_device())\n",
    "\n",
    "text_outputs = model(**text_inputs).to(get_device())\n",
    "image_outputs = model(**image_inputs).to(get_device())\n",
    "\n",
    "def get_embeddings(outputs):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(outputs, dict):\n",
    "            embeddings = outputs[\"embeddings\"]\n",
    "        else:\n",
    "            embeddings = outputs\n",
    "    emb = normalize(embeddings[0], dim=-1).to(get_device()).tolist()\n",
    "    return emb\n",
    "\n",
    "text_emb = get_embeddings(text_outputs)\n",
    "image_emb = get_embeddings(image_outputs)\n",
    "\n",
    "print(f\"text_emb: {len(text_emb)} first3: {text_emb[:3]}\")\n",
    "print(f\"image_emb: {len(image_emb)} first3: {image_emb[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d524b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_token_id': 50407,\n",
       " 'use_cache': True,\n",
       " 'tie_word_embeddings': False,\n",
       " 'scale_factor': 4,\n",
       " 'additional_vocab_size': 40,\n",
       " 'text_config': {'return_dict': True,\n",
       "  'output_hidden_states': False,\n",
       "  'torchscript': False,\n",
       "  'dtype': None,\n",
       "  'pruned_heads': {},\n",
       "  'tie_word_embeddings': True,\n",
       "  'chunk_size_feed_forward': 0,\n",
       "  'is_encoder_decoder': False,\n",
       "  'is_decoder': False,\n",
       "  'cross_attention_hidden_size': None,\n",
       "  'add_cross_attention': False,\n",
       "  'tie_encoder_decoder': False,\n",
       "  'architectures': None,\n",
       "  'finetuning_task': None,\n",
       "  'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "  'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "  'task_specific_params': None,\n",
       "  'problem_type': None,\n",
       "  'tokenizer_class': None,\n",
       "  'prefix': None,\n",
       "  'bos_token_id': None,\n",
       "  'pad_token_id': None,\n",
       "  'eos_token_id': None,\n",
       "  'sep_token_id': None,\n",
       "  'decoder_start_token_id': None,\n",
       "  'max_length': 20,\n",
       "  'min_length': 0,\n",
       "  'do_sample': False,\n",
       "  'early_stopping': False,\n",
       "  'num_beams': 1,\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 50,\n",
       "  'top_p': 1.0,\n",
       "  'typical_p': 1.0,\n",
       "  'repetition_penalty': 1.0,\n",
       "  'length_penalty': 1.0,\n",
       "  'no_repeat_ngram_size': 0,\n",
       "  'encoder_no_repeat_ngram_size': 0,\n",
       "  'bad_words_ids': None,\n",
       "  'num_return_sequences': 1,\n",
       "  'output_scores': False,\n",
       "  'return_dict_in_generate': False,\n",
       "  'forced_bos_token_id': None,\n",
       "  'forced_eos_token_id': None,\n",
       "  'remove_invalid_values': False,\n",
       "  'exponential_decay_length_penalty': None,\n",
       "  'suppress_tokens': None,\n",
       "  'begin_suppress_tokens': None,\n",
       "  'num_beam_groups': 1,\n",
       "  'diversity_penalty': 0.0,\n",
       "  '_name_or_path': '',\n",
       "  'transformers_version': '4.57.3',\n",
       "  'text_model_name': 'jhu-clsp/ettin-encoder-150m',\n",
       "  'hidden_size': 768,\n",
       "  'num_hidden_layers': 22,\n",
       "  'intermediate_size': 1152,\n",
       "  'mlp_bias': False,\n",
       "  'vocab_size': 50368,\n",
       "  'model_type': 'modernvbert_text',\n",
       "  'tf_legacy_loss': False,\n",
       "  'use_bfloat16': False,\n",
       "  'output_attentions': False},\n",
       " 'vision_config': {'return_dict': True,\n",
       "  'output_hidden_states': False,\n",
       "  'torchscript': False,\n",
       "  'dtype': None,\n",
       "  'pruned_heads': {},\n",
       "  'tie_word_embeddings': True,\n",
       "  'chunk_size_feed_forward': 0,\n",
       "  'is_encoder_decoder': False,\n",
       "  'is_decoder': False,\n",
       "  'cross_attention_hidden_size': None,\n",
       "  'add_cross_attention': False,\n",
       "  'tie_encoder_decoder': False,\n",
       "  'architectures': None,\n",
       "  'finetuning_task': None,\n",
       "  'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "  'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "  'task_specific_params': None,\n",
       "  'problem_type': None,\n",
       "  'tokenizer_class': None,\n",
       "  'prefix': None,\n",
       "  'bos_token_id': None,\n",
       "  'pad_token_id': None,\n",
       "  'eos_token_id': None,\n",
       "  'sep_token_id': None,\n",
       "  'decoder_start_token_id': None,\n",
       "  'max_length': 20,\n",
       "  'min_length': 0,\n",
       "  'do_sample': False,\n",
       "  'early_stopping': False,\n",
       "  'num_beams': 1,\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 50,\n",
       "  'top_p': 1.0,\n",
       "  'typical_p': 1.0,\n",
       "  'repetition_penalty': 1.0,\n",
       "  'length_penalty': 1.0,\n",
       "  'no_repeat_ngram_size': 0,\n",
       "  'encoder_no_repeat_ngram_size': 0,\n",
       "  'bad_words_ids': None,\n",
       "  'num_return_sequences': 1,\n",
       "  'output_scores': False,\n",
       "  'return_dict_in_generate': False,\n",
       "  'forced_bos_token_id': None,\n",
       "  'forced_eos_token_id': None,\n",
       "  'remove_invalid_values': False,\n",
       "  'exponential_decay_length_penalty': None,\n",
       "  'suppress_tokens': None,\n",
       "  'begin_suppress_tokens': None,\n",
       "  'num_beam_groups': 1,\n",
       "  'diversity_penalty': 0.0,\n",
       "  '_name_or_path': '',\n",
       "  'transformers_version': '4.57.3',\n",
       "  'vision_model_name': 'google/siglip2-base-patch16-512',\n",
       "  'embed_dim': 768,\n",
       "  'image_size': 512,\n",
       "  'patch_size': 16,\n",
       "  'num_hidden_layers': 12,\n",
       "  'intermediate_size': 3072,\n",
       "  'model_type': 'modernvbert_vision',\n",
       "  'tf_legacy_loss': False,\n",
       "  'use_bfloat16': False,\n",
       "  'output_attentions': False},\n",
       " 'freeze_config': {'freeze_lm_head': True,\n",
       "  'freeze_text_layers': True,\n",
       "  'freeze_vision_layers': True},\n",
       " 'pixel_shuffle_factor': 4,\n",
       " 'use_resampler': False,\n",
       " 'neftune_noise_alpha': 0.0,\n",
       " 'initializer_range': 0.02,\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'torchscript': False,\n",
       " 'dtype': torch.float32,\n",
       " '_output_attentions': False,\n",
       " 'pruned_heads': {},\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'architectures': ['BiModernVBert'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': None,\n",
       " 'pad_token_id': None,\n",
       " 'eos_token_id': None,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " '_name_or_path': 'ModernVBERT/modernvbert-embed',\n",
       " '_commit_hash': 'a24fee187c5ae7f354150ac87e6e874d25d512d5',\n",
       " '_attn_implementation_internal': 'sdpa',\n",
       " 'transformers_version': None,\n",
       " 'auto_map': {'AutoConfig': 'configuration_modernvbert.ModernVBertConfig',\n",
       "  'AutoModel': 'modeling_modernvbert.ModernVBertModel',\n",
       "  'AutoModelForMaskedLM': 'modeling_modernvbert.ModernVBertForMaskedLM'},\n",
       " 'max_position_embeddings': 8192,\n",
       " 'model_type': 'modernvbert',\n",
       " 'qk_layer_norms': False,\n",
       " 'vocab_size': 50368,\n",
       " 'hidden_size': 768,\n",
       " 'tf_legacy_loss': False,\n",
       " 'use_bfloat16': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366175b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Other text\"\n",
    "text_inputs = processor.process_texts([text, text2]).to(get_device())\n",
    "text_outputs = model(**text_inputs).to(get_device())\n",
    "text_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Other text\"\n",
    "text3 = \"Other text\"\n",
    "text_inputs = processor.process_texts([text, text2, text3]).to(get_device())\n",
    "text_outputs = model(**text_inputs).to(get_device())\n",
    "text_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0333675",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Other text\"\n",
    "text3 = \"Other text\"\n",
    "text4 = \"Other text\"\n",
    "text_inputs = processor.process_texts([text, text2, text3, text4]).to(get_device())\n",
    "text_outputs = model(**text_inputs).to(get_device())\n",
    "text_outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec8fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_embeddings(outputs):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tif isinstance(outputs, dict):\n",
    "\t\t\t\tembeddings = outputs[\"embeddings\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\tembeddings = outputs\n",
    "\t\t\tvectors = []\n",
    "\t\t\tfor emb in embeddings:\n",
    "\t\t\t\tvectors.append(normalize(emb, dim=-1).to(get_device()).tolist())\n",
    "\t\t\treturn vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf5127f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = _get_embeddings(text_outputs)\n",
    "len(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models from Config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
